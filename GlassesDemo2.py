import re
import os
import h5py
import logging
import torch
import numpy as np
import pandas as pd
from datetime import datetime
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import faiss
import json
import time

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

model_name = 'paraphrase-multilingual-mpnet-base-v2'
GoldenDataTest = "./DataSets/GoldenTestSet.json"

# ================== 1. å‘é‡åŒ–ç±»ï¼ˆå« Faiss æ¥å£ï¼‰==================
class Vectorizer:
    def __init__(self, model_name='paraphrase-multilingual-mpnet-base-v2'):
        #è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
        if torch.backends.mps.is_available():
            device = "mps"
        elif torch.cuda.is_available():
            device = "cuda"
        else:
            device = "cpu"
        device = "cpu" #Faisså¯èƒ½åªæ”¯æŒCPU

        self.model = SentenceTransformer(model_name, device=device)
        self.device = device
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}ï¼ˆè®¾å¤‡: {self.device}ï¼‰")

    def batch_encode(self, texts, batch_size=64, normalize=True):
        """æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            emb = self.model.encode(batch,
                                    convert_to_numpy=True,
                                    normalize_embeddings=normalize,
                                    show_progress_bar=True)
            embeddings.append(emb)
        return np.vstack(embeddings)


# ================== 2. Faiss æ¥å£å°è£…ï¼ˆå¯æ›¿æ¢å…¶ä»–å¼•æ“ï¼‰==================
class FaissIndexer:
    @staticmethod
    def build(vectors):
        """æ„å»ºç´¢å¼•ï¼ˆé€šç”¨æ¥å£ï¼‰"""
        dim = vectors.shape[1]
        index = faiss.IndexFlatIP(dim)  # ä½¿ç”¨å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(vectors)  # å½’ä¸€åŒ–å¤„ç†
        index.add(vectors)
        return index

    @staticmethod
    def save(index, file_path):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(index, file_path)

    @staticmethod
    def load(file_path):
        """åŠ è½½ç´¢å¼•"""
        return faiss.read_index(file_path)


# ================== 3. æ•°æ®å¤„ç†ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰==================
def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
    if isinstance(text, str):
        text = re.sub(r"\s+", " ", text)
        text = re.sub(r"[^\w\s]", "", text)
        return text.strip()
    return ""


def read_file():
    """è¯»å– CSV æ–‡ä»¶ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
    df = pd.read_csv("./DataSets/testTrainData.csv",
                     delimiter=",",
                     encoding="utf-8-sig",
                     index_col=0)
    df.columns = df.columns.str.strip()

    for col in df.columns:
        df[col] = df[col].apply(
            lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)

    return df


# ================== 4. HDF5 ç¼“å­˜ï¼ˆé›†æˆç´¢å¼•å­˜å‚¨ï¼‰==================
HDF5_FILENAME = f"./DataSets/processed_data_{model_name}.h5"


def save_cache(df, vectors):
    """ä¿å­˜æ•°æ®å’Œç´¢å¼•ï¼ˆæ–°å¢ç´¢å¼•å­˜å‚¨ï¼‰"""
    with h5py.File(HDF5_FILENAME, "w") as f:
        # å­˜å‚¨å‘é‡
        f.create_dataset("embeddings", data=vectors)

        # å­˜å‚¨å…ƒæ•°æ®
        metadata_group = f.create_group("metadata")
        for col in df.columns:
            str_dtype = h5py.string_dtype(encoding='utf-8')
            data = df[col].astype(str).values.astype(str_dtype)
            metadata_group.create_dataset(col, data=data)

        # å­˜å‚¨ Faiss ç´¢å¼•
        index = FaissIndexer.build(vectors)
        FaissIndexer.save(index, "temp.index")
        with open("temp.index", "rb") as f_index:
            f.create_dataset("faiss_index", data=np.void(f_index.read()))

        # è®°å½•æ—¶é—´æˆ³
        f.attrs["create_time"] = datetime.now().isoformat()

    os.remove("temp.index")
    logger.info(f"âœ… æ•°æ®+ç´¢å¼•å·²ç¼“å­˜åˆ° {HDF5_FILENAME}")


def load_cache():
    """åŠ è½½ç¼“å­˜ï¼ˆè¿”å›æ•°æ®+ç´¢å¼•ï¼‰"""
    if os.path.exists(HDF5_FILENAME):
        with h5py.File(HDF5_FILENAME, "r") as f:
            logger.info("âœ… åŠ è½½ç¼“å­˜æ•°æ®...")

            # åŠ è½½æ•°æ®
            df = pd.DataFrame({col: list(f["metadata"][col])
                               for col in f["metadata"].keys()})
            vectors = np.array(f["embeddings"])

            # åŠ è½½ç´¢å¼•
            index_data = bytes(f["faiss_index"][()])
            with open("temp.index", "wb") as f_temp:
                f_temp.write(index_data)
            index = FaissIndexer.load("temp.index")
            os.remove("temp.index")

            return df, vectors, index
    return None, None, None


# ================== 5. ä¸»å¤„ç†æµç¨‹ï¼ˆè¿”å›ç´¢å¼•ï¼‰==================
def process_data(clean_cache=False):
    """ä¸»å¤„ç†æµç¨‹ï¼ˆè¿”å›ç´¢å¼•ï¼‰"""
    if clean_cache:
        if os.path.exists(HDF5_FILENAME):
            os.remove(HDF5_FILENAME)
        logger.info("ğŸ—‘ï¸ ç¼“å­˜å·²æ¸…é™¤ï¼Œé‡æ–°è®¡ç®—...")

    # å°è¯•åŠ è½½ç¼“å­˜
    cached_df, cached_vectors, cached_index = load_cache()
    if cached_df is not None:
        logger.info("âœ… æˆåŠŸåŠ è½½ç¼“å­˜")
        return cached_df, cached_vectors, cached_index

    # å¤„ç†æ–°æ•°æ®
    df = read_file()
    text_cols = [col for col in df.columns if df[col].dtype == "object"]

    for col in text_cols:
        df[col] = df[col].fillna("").apply(clean_text)

    df["text"] = df[text_cols].apply(lambda x: " ".join(x), axis=1)

    vectorizer = Vectorizer()
    vectors = vectorizer.batch_encode(df["text"].tolist())

    save_cache(df, vectors)
    _, _, index = load_cache()  # ç¡®ä¿ç´¢å¼•åŠ è½½

    return df, vectors, index


# ================== 6. éªŒè¯å‡½æ•°ï¼ˆæ–°å¢ç´¢å¼•æ£€æŸ¥ï¼‰==================
def validate_hdf5_file(original_df, original_vectors):
    """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§ï¼ˆæ–°å¢ç´¢å¼•æ£€æŸ¥ï¼‰"""
    if not os.path.exists(HDF5_FILENAME):
        raise FileNotFoundError(f"{HDF5_FILENAME} ä¸å­˜åœ¨")

    with h5py.File(HDF5_FILENAME, "r") as f:
        # åŸºç¡€æ£€æŸ¥
        assert "embeddings" in f, "ç¼ºå°‘å‘é‡æ•°æ®é›†"
        assert "metadata" in f, "ç¼ºå°‘å…ƒæ•°æ®ç»„"
        assert "faiss_index" in f, "ç¼ºå°‘Faissç´¢å¼•"

        # ç»´åº¦æ£€æŸ¥
        assert f["embeddings"].shape == original_vectors.shape, "å‘é‡ç»´åº¦ä¸åŒ¹é…"

        # åˆ—åæ£€æŸ¥
        metadata_cols = list(f["metadata"].keys())
        assert metadata_cols == list(original_df.columns), "åˆ—åä¸åŒ¹é…"

        # ç¼–ç æ£€æŸ¥
        for col in metadata_cols:
            assert f["metadata"][col].dtype == h5py.string_dtype(encoding='utf-8'), f"åˆ— {col} ç¼–ç é”™è¯¯"

    logger.info("âœ… HDF5æ–‡ä»¶éªŒè¯é€šè¿‡")


# ================== 7. å¯è§†åŒ–ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰==================
def visualize_similarity(df, vectors, sample_size=50):
    """å¯è§†åŒ–ç›¸ä¼¼åº¦ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰"""
    if len(df) > sample_size:
        df_sample = df.sample(sample_size, random_state=42)
        vectors_sample = vectors[df_sample.index]
    else:
        df_sample = df
        vectors_sample = vectors

    sim_matrix = np.dot(vectors_sample, vectors_sample.T)
    labels = df_sample["text"].str.slice(0, 50).tolist()  # æˆªæ–­é•¿æ–‡æœ¬

    plt.figure(figsize=(12, 10))
    sns.heatmap(sim_matrix,
                xticklabels=labels,
                yticklabels=labels,
                cmap="viridis")
    plt.title("æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ")
    plt.show()


# ================== 8. è¯­ä¹‰æœç´¢æ¥å£ ==================
def semantic_search(query_texts, vectorizer, index, df, top_k=5):
    """é€šç”¨æœç´¢æ¥å£"""
    query_vectors = vectorizer.batch_encode(query_texts)
    faiss.normalize_L2(query_vectors)

    # æ‰§è¡Œæœç´¢
    scores, indices = index.search(query_vectors, top_k)

    # æ•´ç†ç»“æœ
    results = []
    for q_text, q_scores, q_indices in zip(query_texts, scores, indices):
        result = {
            "query": q_text,
            "results": []
        }
        for score, idx in zip(q_scores, q_indices):
            result["results"].append({
                "text": df.iloc[idx]["text"],
                "score": float(score)
            })
        results.append(result)
    return results


# ================== 10. æ•°æ®åŠ è½½å’Œè¯„ä¼° ==================
def load_data(file_path):
    """åŠ è½½ GoldenTest.json æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def parse_text_to_features(text):
    """å°†æ–‡æœ¬è§£æä¸ºç‰¹å¾å­—å…¸"""
    features = {}
    # å‡è®¾æ–‡æœ¬æ ¼å¼ä¸º "ç‰¹å¾1:å€¼1 ç‰¹å¾2:å€¼2"
    pairs = re.findall(r"(\w+):([^ ]+)", text)
    for key, value in pairs:
        features[key.strip()] = value.strip()
    return features

def compute_precision_recall(ground_truth, rag_recommended, text_columns):
    """è®¡ç®—å‡†ç¡®ç‡ï¼ˆPrecisionï¼‰å’Œå¬å›ç‡ï¼ˆRecallï¼‰"""
    total_precision = 0
    total_recall = 0
    count = 0

    for test_case in ground_truth:
        input_features = test_case["features"]
        input_filtered = {k: v for k, v in input_features.items() if v != "nil"}
        
        if not input_filtered:
            continue

        # è·å–æ¨èç»“æœ
        recommended_glasses = rag_recommended
        
        # è§£ææ¨èæ–‡æœ¬ä¸ºç‰¹å¾å­—å…¸ï¼ˆæ ¹æ®åˆ—åé¡ºåºï¼‰
        recommended_features = []
        for recKey, value in recommended_glasses:
            try:
                # æŒ‰åˆ—é¡ºåºè§£æç‰¹å¾å€¼
                values = value[0]["text"].split()
                features = {col: values[i] for i, col in enumerate(text_columns)}
                recommended_features.append(features)
            except Exception as e:
                logger.warning(f"ç‰¹å¾è§£æå¤±è´¥: {rec} - {str(e)}")
                continue

        # è®¡ç®—åŒ¹é…æ•°
        perfect_matches = 0
        for rec_features in recommended_features:
            # æ£€æŸ¥æ‰€æœ‰ç‰¹å¾æ˜¯å¦å®Œå…¨åŒ¹é…
            match = all(
                rec_features.get(key, "").lower() == value.lower()
                for key, value in input_filtered.items()
            )
            if match:
                perfect_matches += 1

        # è®¡ç®—æŒ‡æ ‡
        recommended_count = len(recommended_glasses)
        precision = perfect_matches / recommended_count if recommended_count > 0 else 0
        recall = perfect_matches / 1  # æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹åªæœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆ
        
        total_precision += precision
        total_recall += recall
        count += 1

    avg_precision = total_precision / count if count else 0
    avg_recall = total_recall / count if count else 0
    return avg_precision, avg_recall


# ================== 11. ä¸»æ‰§è¡Œé€»è¾‘ ==================
if __name__ == "__main__":
    # å¤„ç†æ•°æ®ï¼ˆè¿”å›ç´¢å¼•ï¼‰
    df, vectors, faiss_index = process_data()
    
    # è·å–æ–‡æœ¬åˆ—åï¼ˆä¿æŒä¸ç”Ÿæˆtextæ—¶ç›¸åŒçš„é¡ºåºï¼‰
    text_columns = [col for col in df.columns if df[col].dtype == "object"] 

    # éªŒè¯æ–‡ä»¶
    validate_hdf5_file(df, vectors)

    # æ‰“å°åŸºæœ¬ä¿¡æ¯
    print(f"\næ•°æ®æ¦‚å†µ:\n{df.dtypes}\n")
    print(f"ç¤ºä¾‹æ–‡æœ¬:\n{df['text'].head()}\n")
    print(f"å‘é‡ç»´åº¦: {vectors.shape}\n")

    # åŠ è½½æµ‹è¯•æ•°æ®
    ground_truth = load_data(GoldenDataTest)

    # æ£€æŸ¥æ¨èç»“æœç¼“å­˜
    recommendation_file = "recommendation_results.csv"
    if os.path.exists(recommendation_file):
        logger.info("âœ… æ£€æµ‹åˆ°æ¨èç»“æœç¼“å­˜ï¼Œç›´æ¥åŠ è½½...")
        results_df = pd.read_csv(recommendation_file)
        # æ„å»ºæ¨èç»“æœæ•°æ®ç»“æ„
        first_recommend_result = {}
        all_recommend_results = []
        for query, group in results_df.groupby('query'):
            # å–æ¯ä¸ªæŸ¥è¯¢çš„ç¬¬ä¸€ä¸ªæ¨èç»“æœ
            first_rec = group.iloc[0].to_dict()
            first_recommend_result[query] = [{
                "text": first_rec["recommended_text"],
                "score": first_rec["score"]
            }]
            # æ”¶é›†æ‰€æœ‰ç»“æœ
            all_recommend_results.extend(group.to_dict('records'))
    else:
        # åˆå§‹åŒ– Vectorizer
        vectorizer = Vectorizer()
        first_recommend_result = {}
        all_recommend_results = []
        start_time = time.time()
        
        for test_case in ground_truth:
            input_text = test_case["input"]
            results = semantic_search([input_text], vectorizer, faiss_index, df)
            
            if results and results[0]["results"]:
                first_recommend_result[input_text] = [results[0]["results"][0]]
            
            for result in results:
                for rec in result["results"]:
                    all_recommend_results.append({
                        "query": result["query"],
                        "recommended_text": rec["text"],
                        "score": rec["score"]
                    })

        # ä¿å­˜ç»“æœ
        results_df = pd.DataFrame(all_recommend_results)
        results_df.to_csv(recommendation_file, index=False, encoding='utf-8-sig')
        logger.info("âœ… æ¨èç»“æœå·²ä¿å­˜åˆ° recommendation_results.csv")

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    avg_precision, avg_recall = compute_precision_recall(
        ground_truth, 
        first_recommend_result.items(),
        text_columns
    )

    # è®°å½•è¿è¡Œæ—¶é—´
    end_time = time.time()  # End timing
    execution_time = end_time - start_time

    # æ‰“å°è¯„ä¼°ç»“æœ
    print(f"\nå¹³å‡å‡†ç¡®ç‡: {avg_precision:.4f}")
    print(f"å¹³å‡å¬å›ç‡: {avg_recall:.4f}")
    # è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶
    with open("execution_time_log.txt", "a") as log_file:
        log_file.write(f"Execution Time: {execution_time:.2f} seconds\n")
        log_file.write(f"Average Precision: {avg_precision:.4f}\n")
        log_file.write(f"Average Recall: {avg_recall:.4f}\n")

